# -*- coding: utf-8 -*-
"""LSTM_Sentiment-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BQLWepLL91_dBYigUqE-jU4R5zUKZeM8
"""

!pip install nltk
#!pip install pyspellchecker

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
nltk.download(["punkt","wordnet"])
#from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import re
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
#from spellchecker import SpellChecker
from gensim import corpora
from collections import Counter
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset


df= pd.read_csv("/content/drive/MyDrive/IMDB Dataset.csv") # specify your own file path  
df.head()

df["sentiment"].unique()

"""Hence, the sentiment is either postive or negative, we need to convert these values to binary values to feed it into our model"""

df["sentiment"]= pd.Series(np.where(df["sentiment"]=="positive",1,0))

df["sentiment"].value_counts().plot.bar(title= "Sentiment Distribution in the dataset")
plt.xlabel("Sentiments")
plt.ylabel("Number of Examples")
plt.show()

# def get_data(top_n):
#     df_positive = df[df['sentiment'] == 1].head(top_n)
#     df_negative = df[df['sentiment'] == 0].head(top_n)
#     df_small = pd.concat([df_positive, df_negative])
#     return df_small

# df_small = get_data(5000)
# df_small.head()

"""## Text Preprocessing:-

1. Removing punctuations, symbols, html tags
2. lemmitisation, Spelling Correction
3. Tokenisation

### Something yet to implement:--
*  Removing numbers

### Why do you need to preprocess this text?
Not all the information is useful in making predictions or doing classifications. Reducing the number of words will reduce the input dimension to your model. The way the language is written, it contains lot of information which is grammar specific. Thus when converting to numeric format, word specific characteristics like capitalisation, punctuations, suffixes/prefixes etc. are redundant. Cleaning the data in a way that similar words map to single word and removing the grammar relevant information from text can tremendously reduce the vocabulary. 

Which methods to apply and which ones to skip depends on the problem at hand.
"""

TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S+|[^A-Za-z0-9]+"

"""Lowercasing and removing all sorts of punctuations including html tags, urls"""

def clean_text(text):
  text = re.sub(TEXT_CLEANING_RE,' ', str(text).lower()).strip()
  tokens = []
  for token in text.split():
    tokens.append(token)
  return " ".join(tokens)

# df_small["review"]= df_small["review"].apply(lambda text: clean_text(text))
df["review"]= df["review"].apply(lambda text: clean_text(text))

lemma= WordNetLemmatizer()
def lemmatize_text(text):
  return " ".join([lemma.lemmatize(word) for word in text.split()])

#df_small["lemmatised_text"]= df_small["review"].apply(lambda text: lemmatize_text(str(text)))
df["lemmatised_text"]= df["review"].apply(lambda text: lemmatize_text(text))

df["lemmatised_text"].head()

# word_tokenize is pre-define function in nltk.tokenize
df["tokenized_text"]=df["lemmatised_text"].apply(lambda text: word_tokenize(text))
df["tokenized_text"]

#def stem(text):
  #ps=PorterStemmer()
  #return " ".join([ps.stem(word) for word in text.split()])

#df_small["review"]= df_small["review"].apply(lambda text: stem(text))

#spell= SpellChecker()
#def spellcheck(text):
  #misspelled_word= spell.unknown(text.split())
  #return " ".join([spell.correction(word) if word in misspelled_word else word for word in text.split()])

#df_small["review"]= df_small["review"].apply(lambda text: spellcheck(text))

"""creating vocabulary for index based dictionary mapping of all the words."""

words=[]
for index, row in df[["tokenized_text"]].iterrows():
  for i in range(len(row["tokenized_text"])):
    words.append(row["tokenized_text"][i])
count_words= Counter(words)
sorted_words = sorted(count_words,key=count_words.get,reverse=True)

vocab_to_int={w:i+1 for i,w in enumerate(sorted_words)}

"""Tokenize the reviews to get the final integer mapping."""

review_int=[]
for index,row in df[["tokenized_text"]].iterrows():
  review_int.append([vocab_to_int[x] for x in row["tokenized_text"]])

review_length= [len(i) for i in review_int]
pd.Series(review_length).hist()
plt.show()
pd.Series(review_length).describe()

if torch.cuda.is_available():
  device= torch.device("cuda")
else:
  device= torch.device("cpu")

print(device)

def make_target(label):
    if label == 1:
        return torch.tensor([1], dtype=torch.long, device=device)
    else:
        return torch.tensor([0], dtype=torch.long, device=device)

df["sentiment"]=df["sentiment"].apply(lambda x: make_target(x))

"""To deal with both short and long reviews, we will pad or truncate all our reviews to a specific length.Lets call it pad_len. This sequence length is same as number of time steps for LSTM layer.

*  For reviews shorter than pad_len, we will pad with zeros. For reviews longer than pad_len we will truncate them to the first 500 words, as majority of the sentences are of length less than 500 as could be seen from the plot

"""

def padding(sentence,pad_len):
  if len(sentence)<=pad_len:
    sentence=sentence+list(np.zeros(pad_len-len(sentence),dtype=int))
  else:
    sentence=sentence[0:pad_len]
  return sentence

review_int=[padding(x,500) for x in review_int] # Majority of review sentences have more than 500 words

"""Splitting into train and test set."""

df['sentiment'].values

# since review_int is a list, we pass the ground truth sentiment score also as a list into train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(review_int,df['sentiment'].values.astype("int64"),shuffle=True,test_size=0.2,random_state=10)

batch_size=32

# creating Tensor Datasets
train_data= TensorDataset(torch.from_numpy(np.array(X_train)),torch.from_numpy(np.array(Y_train)))
test_data= TensorDataset(torch.from_numpy(np.array(X_test)),torch.from_numpy(np.array(Y_test)))

train_loader= DataLoader(train_data,shuffle=True,batch_size=batch_size)
test_loader= DataLoader(test_data,shuffle=True,batch_size=batch_size)

"""Obtaining one batch for manual check:"""

dataiter= iter(train_loader)
sample_x,sample_y=dataiter.next()

print("Sample input:\n",sample_x)
print("\n")
print("Sample label:\n",sample_y)

class LSTM_Sentiment_Analysis(nn.Module):
  def __init__(self,vocab_size,output_size,embedding_size,hidden_dim,n_layer,drop_prob=0.4):
    super().__init__()

    self.output_size=output_size
    self.hidden_dim= hidden_dim
    self.n_layer= n_layer

# For word to vector embedding can use pretrained models like Glove(stanford opensource)
    self.embedding= nn.Embedding(vocab_size,embedding_size) # Have used the untrained embedding neural network
    self.lstm= nn.LSTM(embedding_size,hidden_dim,n_layers,dropout=drop+prob,batch_first=True)


    #Dropout Layer
    self.dropout= nn.Dropout(drop_prob)

    #Fully connected Layer
    self.fc= nn.Linear(hidden_dim,output_size)
    self.nonlinearity= nn.Sigmoid()

  def forward(self,x,hidden):
    
    batch_size= x.size(0)
    embeds= self.embedding(x)
    lstm_out,hidden= self.lstm(embeds,hidden)

    lstm_out= lstm_out.contiguous().view(-1,self.hidden_dim)


    out= self.droput(lstm_out)
    out = self.fc(out)
    sig_out= self.nonlinearity(out)

    sig_out= sig_out.view(batch_size,-1)[:,-1]

    return sig_out,hidden

  
  def init_hidden(self,batch_size):

    weight = next(self.parameters()).data
    hidden = (weight.new(self.n_layer, batch_size, self.hidden_dim).zero_().to(device),
              weight.new(self.n_layer, batch_size, self.hidden_dim).zero_().to(device))
        
    return hidden

"""We can use BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example."""

vocab_size= len(vocab_to_int)+1 # 1 added extra for 0 padding
hidden_dim= 512
output_size=1
n_layer=2
embedding_size=400


model= LSTM_Sentiment_Analysis(vocab_size,output_size,embedding_size,hidden_dim,n_layer)
model.to(device)
print(model)

learning_rate=0.005
loss_function= nn.BCELoss()
optimizer= optim.Adam(model.parameters(), lr=learning_rate)

#training params
epochs = 4
counter=0
print_every=100
clip=5 # gradient clipping

model.train()

for epoch in range(epochs):
  #Initialising the hidden state.
  h= model.init_hidden(batch_size)

  #batch looping
  for inputs, labels in train_loader:
    counter+=1
    h= tuple([m.data for m in h])
    inputs,labels= inputs.to(device), labels.to(device)
    model.zero_grad()
    output, h= model(inputs,h)
    loss= criterion(output.squeeze(),labels.float())
    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), clip)
    optimizer.step()
        
    if counter%print_every == 0:
      val_h = model.init_hidden(batch_size)
      val_losses = []
      model.eval()
      for inp, lab in val_loader:
        val_h = tuple([each.data for each in val_h])
        inp, lab = inp.to(device), lab.to(device)
        out, val_h = model(inp, val_h)
        val_loss = criterion(out.squeeze(), lab.float())
        val_losses.append(val_loss.item())
                
       model.train()
       print("Epoch: {}/{}...".format(i+1, epochs),
            "Step: {}...".format(counter),
          "Loss: {:.6f}...".format(loss.item()),
            "Val Loss: {:.6f}".format(np.mean(val_losses)))
         if np.mean(val_losses) <= valid_loss_min:
           torch.save(model.state_dict(), './state_dict.pt')
           print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))
             valid_loss_min = np.mean(val_losses)

"""Some improvement that can be done:---
* Running a hyperparameter search to optimize.
* Increasing the model complexity
* Adding more layers/using bidirectional LSTMs. Using pre-trained word embeddings such as GloVe embeddings
"""