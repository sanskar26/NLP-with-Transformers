# NLP-with-Transformers
### Project part of Summer of Code IIT Bombay, 2021
\
The first week started with getting familiar with `pytorch` by implementing basic tensor operations and a feed forward neural network for classification task on CIFAR-dataset. Along, with that we got introduced to a very powerful text processing library in python which is **nltk** , it provides utility functions for data preprocessing/cleaning task such as removing stopwords,punctuations, Stemming and Lemmatization task , tokenisation of corpus, etc.I was already familiar with _Neural Networks_ and Backpropagation(the technique used to update the parameters of the model using gradient descent optimisation algorithm) and it helped a lot for getting a practical flavour of its implementation on pytorch. Since a computer program cannot take string/word as input , hence the corpus of text used in training must be converted into numeric form which is done using word vector or word embedding. 
\
<br/>
For the first implementation task, we were supposed to code a feed forward neural net model for Sentiment analysis( Classification Task) on [IMDB moview review dataseet](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) from kaggle. I started by first importing and conducting Exploratory data Analysis(EDA) and got the gist of the data distribution,and found that the target classes where equally distributed, hence we didn't need to undersample or oversample the data instances for an accurate model. Then used `nltk` and `re` library for preprocessing task, Started with a 2 hidden layer neural net with 500 neurons each and used the cross entropy loss function and Adaptive moment estimation(adam) optmiser for back propagation of the loss function updating the parameters of the network. The f1 score was not too pleasing, obviously it required _hyperparameter tuning_, i haven't yet implemented cross validation, which i think could be really helpful to get better results. I finally resorted to 3 hidden layer model with a **softmax** activation function in the output layer. 
