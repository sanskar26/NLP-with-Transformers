{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_FeedForward_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otw6E8CLnUM7",
        "outputId": "5a08da07-a9e0-407e-9159-2ac48931a9ac"
      },
      "source": [
        "!pip install nltk\n",
        "#!pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "VlsB1YkfnQnh",
        "outputId": "b31b2cc1-d020-4315-d62c-307cbb9843cc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "nltk.download([\"punkt\",\"wordnet\"])\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from spellchecker import SpellChecker\n",
        "from gensim import corpora\n",
        "\n",
        "\n",
        "df= pd.read_csv(\"IMDB Dataset.csv\") \n",
        "df.head()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Wwl9ugnXf-",
        "outputId": "63a1e9f7-48d0-491f-fafa-36ffba5214c2"
      },
      "source": [
        "df[\"sentiment\"].unique()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-TrNXNTl3i"
      },
      "source": [
        "Hence, the sentiment is either postive or negative, we need to convert these values to binary values to feed it into our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slLj3kgioCra"
      },
      "source": [
        "df[\"sentiment\"]= pd.Series(np.where(df[\"sentiment\"]==\"positive\",1,0))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "ZthXvTPhoCn9",
        "outputId": "fa0e2f30-a247-44d8-a0a0-0fe69d5f5ec9"
      },
      "source": [
        "df[\"sentiment\"].value_counts().plot.bar(title= \"Sentiment Distribution in the dataset\")\n",
        "plt.xlabel(\"Sentiments\")\n",
        "plt.show()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "RabqWOwnoBXf",
        "outputId": "fe420892-a4e8-4ea4-fcad-98082f893a3d"
      },
      "source": [
        "def get_data(top_n = 5000):\n",
        "    df_positive = df[df['sentiment'] == 1].head(top_n)\n",
        "    df_negative = df[df['sentiment'] == 0].head(top_n)\n",
        "    df_small = pd.concat([df_positive, df_negative])\n",
        "    return df_small\n",
        "\n",
        "df_small = get_data(top_n=5000)\n",
        "\n",
        "df_small.head()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
              "5  Probably my all-time favorite movie, a story o...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LL0t2B5TuFW"
      },
      "source": [
        "As we can see the dataset is quite balanced with almost equal number of data for both the target variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sspPlKLtT1L5"
      },
      "source": [
        "## Text Preprocessing:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om6u4bKjTzr-"
      },
      "source": [
        "1. Removing punctuations,symbols,html tags\n",
        "2. lemmitisation, Spelling Correction\n",
        "3. Tokenisation\n",
        "##### left possibility- removing numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZkVUnBLUZmZ"
      },
      "source": [
        "### Why do you need to preprocess this text?\n",
        "Not all the information is useful in making predictions or doing classifications. Reducing the number of words will reduce the input dimension to your model. The way the language is written, it contains lot of information which is grammar specific. Thus when converting to numeric format, word specific characteristics like capitalisation, punctuations, suffixes/prefixes etc. are redundant. Cleaning the data in a way that similar words map to single word and removing the grammar relevant information from text can tremendously reduce the vocabulary. Which methods to apply and which ones to skip depends on the problem at hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G77tK6TWoCdh"
      },
      "source": [
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S+|[^A-Za-z0-9]+\""
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUJnS7qkUjn9"
      },
      "source": [
        "Lowercasing and removing all sorts of punctuations including html tags, urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3PIwIWupl7h"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub(TEXT_CLEANING_RE,' ', str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    tokens.append(token)\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "df_small[\"review\"]= df_small[\"review\"].apply(lambda text: clean_text(text))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mRG4jeMSBTR"
      },
      "source": [
        "lemma= WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "  return \" \".join([lemma.lemmatize(word) for word in text.split()])\n",
        "\n",
        "df_small[\"lemmatised_text\"]= df_small[\"review\"].apply(lambda text: lemmatize_text(str(text)))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_rR5YS7q4kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad359202-f61e-4848-a869-90ec694916eb"
      },
      "source": [
        "df_small[\"tokenized_text\"]=df_small[\"lemmatised_text\"].apply(lambda text: word_tokenize(text))\n",
        "df_small[\"tokenized_text\"]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [one, of, the, other, reviewer, ha, mentioned,...\n",
              "1        [a, wonderful, little, production, br, br, the...\n",
              "2        [i, thought, this, wa, a, wonderful, way, to, ...\n",
              "4        [petter, mattei, s, love, in, the, time, of, m...\n",
              "5        [probably, my, all, time, favorite, movie, a, ...\n",
              "                               ...                        \n",
              "10038    [it, surprisingly, had, a, plot, i, ve, seen, ...\n",
              "10039    [i, suppose, i, m, supposed, to, take, somethi...\n",
              "10044    [this, is, strictly, for, pryor, fan, just, be...\n",
              "10047    [the, only, saving, grace, of, this, movie, is...\n",
              "10048    [this, may, contain, spoiler, br, br, where, t...\n",
              "Name: tokenized_text, Length: 10000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2uHKJGAq4Yq"
      },
      "source": [
        "#def stem(text):\n",
        "  #ps=PorterStemmer()\n",
        "  #return \" \".join([ps.stem(word) for word in text.split()])\n",
        "\n",
        "#df_small[\"review\"]= df_small[\"review\"].apply(lambda text: stem(text))\n",
        "\n",
        "#spell= SpellChecker()\n",
        "#def spellcheck(text):\n",
        "  #misspelled_word= spell.unknown(text.split())\n",
        "  #return \" \".join([spell.correction(word) if word in misspelled_word else word for word in text.split()])\n",
        "\n",
        "#df_small[\"review\"]= df_small[\"review\"].apply(lambda text: spellcheck(text))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JNOKj7ZVMFT"
      },
      "source": [
        "Splitting into train and test set:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uKjLVeWq4V2"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(df_small[['tokenized_text']], df_small['sentiment'],shuffle=True,test_size=0.2,random_state=10)\n",
        "\n",
        "X_train = X_train.reset_index()\n",
        "X_test = X_test.reset_index()\n",
        "Y_train = Y_train.to_frame()\n",
        "Y_train = Y_train.reset_index()\n",
        "Y_test = Y_test.to_frame()\n",
        "Y_test = Y_test.reset_index()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-r08a0gq4T7",
        "outputId": "15fb9157-7aeb-4716-a114-876a262d6b9a"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device= torch.device(\"cuda\")\n",
        "else:\n",
        "  device= torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqDy-QBFV1Rt"
      },
      "source": [
        "## Defining the Feed forward model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ktz1EtV420"
      },
      "source": [
        "I have taken  hidden layer with different neurons. And to start with have considered Relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns1zerd1sOQd"
      },
      "source": [
        "class FeedforwardNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNN, self).__init__()\n",
        "        \n",
        "        # First Hidden Layer:-\n",
        "        self.f1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Adding Non-linearity \n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Second Hidden Layer:-\n",
        "        self.f2 = nn.Linear(hidden_dim, hidden_dim) \n",
        "        # Non-linearity 2\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Third Hidden Layer:-\n",
        "        self.f3 = nn.Linear(hidden_dim, hidden_dim) \n",
        "        # Non-linearity 3\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Output Layer\n",
        "        self.f4 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        # Linear function 1\n",
        "        out = self.f1(x)\n",
        "        # Non-linearity 1\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.f2(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Linear function 3\n",
        "        out = self.f3(out)\n",
        "        # Non-linearity 3\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        #output\n",
        "        out = self.f4(out)\n",
        "\n",
        "        return F.softmax(out, dim=1)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR979j_krHpc"
      },
      "source": [
        "# Function to return the dictionary\n",
        "def make_dict(df_small):\n",
        "  vocab_dict = corpora.Dictionary(df_small[\"tokenized_text\"])\n",
        "  return vocab_dict\n",
        "\n",
        "# Make the dictionary:-\n",
        "vocab_dict = make_dict(df_small)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93wCD5b0rHl2"
      },
      "source": [
        "Vocab_size = len(vocab_dict)\n",
        "# Function to make bow vector to be used as input to network\n",
        "def make_bow_vector(vocab_dict, sentence):\n",
        "  vec = torch.zeros(Vocab_size, dtype=torch.float64, device=device)\n",
        "  for word in sentence:\n",
        "      vec[vocab_dict.token2id[word]] += 1\n",
        "  return vec.view(1, -1).float()"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK6zfgHMrHj4"
      },
      "source": [
        "def make_target(label):\n",
        "    if label == 1:\n",
        "        return torch.tensor([1], dtype=torch.long, device=device)\n",
        "    else:\n",
        "        return torch.tensor([0], dtype=torch.long, device=device)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anfZZaMMrHhe"
      },
      "source": [
        "input_dim= Vocab_size\n",
        "hidden_dim= 500\n",
        "output_dim=2\n",
        "num_epochs=10\n",
        "learning_rate=0.1\n",
        "# RUN TRAINING AND TEST\n",
        "\n",
        "ff_nn_bow_model= FeedforwardNN(input_dim,hidden_dim,output_dim)\n",
        "ff_nn_bow_model.to(device)\n",
        "\n",
        "loss_function= nn.CrossEntropyLoss()\n",
        "optimizer= optim.Adam(ff_nn_bow_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7LXxXR7rHej"
      },
      "source": [
        "#writing the loss value in a file\n",
        "ffnn_loss_file_name = \"ffnn_loss.csv\"\n",
        "f= open(ffnn_loss_file_name,\"w\")\n",
        "f.write(\"iter.loss\")\n",
        "f.write(\"\\n\")\n",
        "losses=[]\n",
        "iter=0\n",
        "batch_size=100\n",
        "#starting training\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss=0\n",
        "  for index, row in X_train.iterrows():\n",
        "    #Clearing the accumulated gradients\n",
        "    # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Make the bag of words vector for stemmed tokens \n",
        "    bow_vec = make_bow_vector(vocab_dict, row[\"tokenized_text\"])\n",
        "      \n",
        "    #Forward pass to get output\n",
        "    probs = ff_nn_bow_model(bow_vec)\n",
        "\n",
        "    #Getting the target label\n",
        "    target = make_target(Y_train[\"sentiment\"][index])\n",
        "\n",
        "    #Calculate loss\n",
        "    loss = loss_function(probs,target)\n",
        "    \n",
        "    #Accumulating the loss over time\n",
        "    train_loss+=loss.item()\n",
        "    \n",
        "    #Getting gradients wrt paramters:-\n",
        "    loss.backward()\n",
        "\n",
        "    #Updating Paramters\n",
        "    optimizer.step()\n",
        "\n",
        "  f.write(str(epoch+1)+ \",\" + str(train_loss/len(X_train))  )\n",
        "  f.write(\"\\n\")\n",
        "  train_loss=0\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpr3w8GQrHTV"
      },
      "source": [
        "bow_ff_nn_predictions = []\n",
        "original_lables_ff_bow = []\n",
        "with torch.no_grad():\n",
        "    for index, row in X_test.iterrows():\n",
        "        bow_vec = make_bow_vector(vocab_dict, row)\n",
        "        probs = ff_nn_bow_model(bow_vec)\n",
        "        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
        "        original_lables_ff_bow.append(make_target(Y_test[\"sentiment\"][index]).cpu().numpy()[0])\n",
        "print(classification_report(original_lables_ff_bow,bow_ff_nn_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHpSZyw_sAmw"
      },
      "source": [
        "for epoch in range(0, num_epochs):\n",
        "        ex_indices = [i for i in range(0, len(train_ys))]\n",
        "        random.shuffle(ex_indices)\n",
        "        total_loss = 0.0\n",
        "        for idx in ex_indices:\n",
        "            x = form_input(train_xs[idx])\n",
        "            y = train_ys[idx]\n",
        "            # Build one-hot representation of y\n",
        "            y_onehot = torch.zeros(num_classes)\n",
        "            y_onehot.scatter_(0, torch.from_numpy(np.asarray(y,dtype=np.long)), 1)\n",
        "            # Zero out the gradients from the FFNN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
        "            ffnn.zero_grad()\n",
        "            probs = ffnn.forward(x)\n",
        "            # Can also use built-in NLLLoss as a shortcut here (takes log probabilities) but we're being explicit here\n",
        "            loss = torch.neg(torch.log(probs)).dot(y_onehot)\n",
        "            total_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Loss on epoch %i: %f\" % (epoch, total_loss))\n",
        "\n",
        "   train_correct = 0\n",
        "    for idx in range(0, len(train_xs)):\n",
        "        # Note that we only feed in the x, not the y, since we're not training. We're also extracting different\n",
        "        # quantities from the running of the computation graph, namely the probabilities, prediction, and z\n",
        "        x = form_input(train_xs[idx])\n",
        "        y = train_ys[idx]\n",
        "        probs = ffnn.forward(x)\n",
        "        prediction = torch.argmax(probs)\n",
        "        if y == prediction:\n",
        "            train_correct += 1\n",
        "        print(\"Example \" + repr(train_xs[idx]) + \"; gold = \" + repr(train_ys[idx]) + \"; pred = \" +\\\n",
        "              repr(prediction) + \" with probs \" + repr(probs))\n",
        "    print(repr(train_correct) + \"/\" + repr(len(train_ys)) + \" correct after training\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}